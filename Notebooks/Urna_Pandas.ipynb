{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d45845",
   "metadata": {},
   "source": [
    "# Projeto Urna - EDA dos dados das eleições presidenciais de 2022\n",
    "\n",
    "## Sobre o projeto:\n",
    "\n",
    "As eleições presidenciais de 2022 foram algumas das mais intensas que ocorrem desde a redemocratização (tanto o segundo turno quanto o primeiro turno), e assim que os resultados do segundo turno saíram, me senti praticamente intimado a fazer uma análise dos dados das eleições, de ambos os turnos. A princípio, eu quis fazer **apenas** das eleições presidenciais, mas optei no final por fazer para todos os cargos.\n",
    "\n",
    "O dataset das eleições foi obtido no site do [TSE](https://dadosabertos.tse.jus.br/dataset/resultados-2022), vindo juntamente com um arquivo .pdf, que é o dicionário de dados para entendermos o que cada atributo representa. O arquivo original é em formato .csv e a codificação do mesmo é \"Latin-1\" (mas por via de regra, passei como parâmetro \"ISO 8859-1\", que é o \"nome\" oficial da codificação). Eu poderia trabalharcom os dados das eleições envolvendo o estado do Rio de Janeiro inteiro, mas nesta fase vou me ater a cidade onde moro (Nova Friburgo, região serrana do RJ).\n",
    "\n",
    "Como não há microdados (um microdado, segundo a definição do [CETIC](https://www.cetic.br/microdados/), é a menor fração de um dado coletado em pesquisa - ou seja, representa uma pessoa ou uma resposta individual), o número de votos está agrupado no dataset, hierarquicamente como \"estado - cidade - zona eleitoral - seção eleitoral\".\n",
    "\n",
    "Todos os arquivos necessários para a empreitada estarão presentes no mesmo diretório, então zero preocupações quanto a isso. Minha escolha para converter tudo em _.xlsx_ ao fim por ser o formato padrão do Excel (caso haja alguém com vontade de manipular diretamente a planilha.\n",
    "\n",
    "## Informação importante\n",
    "\n",
    "Os datasets das eleições estão separados. Em um dataset estão os resultados das eleições para deputados federais e estaduais, senadores e governadores. Em outro, encontram-se os dados das eleições presidenciais (não entendi muito bem o porquê da separação, mas acredito que a divisão seja critério do TSE). Portanto, o dataset das eleições \"normais\" será chamado apenas de _\"df_votacao\"_ e o dataset das eleições presidenciais será chamado de _'df_votacao_presidente'_. No site do TSE, o dataset presidencial encontra-se com o título _'Presidente - Votação por seção eleitoral - 2022'_ e o dataset 'normal' se encontra com o título _'RJ - Votação por seção eleitoral - 2022'_.\n",
    "\n",
    "Analisando o tamanho dos arquivos - eleição presidencial e eleição normal -, dá pra imaginar o porquê do TSE ter optado por essa divisão. Se fóssemos ter em apenas um arquivo todos os dados das eleições, primeiro e segundo turno para **todos** os cargos, haveria um volume MUITO grande de dados. E também, se pararmos pra pensar de forma lógica, vemos muito mais notícias ligadas a presidência do que da Câmara dos Deputados ou do Senado. Para os analistas de dados que produzem conteúdo para imprensa, é muito mais vantagem ter essa divisão. O trabalho fica mais focado onde deve ficar focado - _apenas estou dando os meus dois centavos sobre o assunto_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9486f",
   "metadata": {},
   "source": [
    "## Importando as bibliotecas\n",
    "\n",
    "Um desafio que coloquei para mim mesmo: Fazer o projeto utilizando DUAS bibliotecas de manipulação de dados. Uma delas é a mais que famosa Pandas e a outra, nem tão conhecida porém ganhando espaço, Polars.\n",
    "Neste espaço, serão usadas APENAS as bibliotecas [Pandas](https://pandas.pydata.org) e [Numpy](https://numpy.org), importadas abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6de256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas que utilizarei por aqui\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "import openpyxl\n",
    "\n",
    "# Para evitar repetições desnecessárias, escrevi uma função que descreve exatamente o que quero: \n",
    "# Que me mostre a quantidade de entradas nulas por coluna e descreva o tipo de dado em cada coluna, além\n",
    "# do formato do dataframe (quantidade de linhas e colunas).\n",
    "\n",
    "def avaliador(df):\n",
    "    formato = df.shape\n",
    "    print(f\"Tamanho do DataFrame selecionado: {formato[1]} colunas e {formato[0]} linhas.\\n\")\n",
    "    print(\"#\" * 20)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Exibindo os tipos de cada coluna\n",
    "    print(\"\\nTIPOS DAS COLUNAS:\")\n",
    "    for coluna in df.columns:\n",
    "        print(f\"Tipo da coluna '{coluna}': {df[coluna].dtype}\")\n",
    "    print(\"\\n\")\n",
    "    print(\"#\" * 20, \"\\n\")\n",
    "\n",
    "    # Quantidade de linhas nulas por coluna\n",
    "    print(\"Quantidade de dados nulos por colunas do DataFrame selecionado: \")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e0674",
   "metadata": {},
   "source": [
    "## Criando o DataFrame com Pandas\n",
    "\n",
    "Aqui é onde importo os dados, no formato .csv, utilizando o método \"read_csv\" e passando como parâmetros o \"encoding\" para cada  arquivo (ISO 8859-1 ou \"Latin-1\"). Também foi necessário determinar o \"delimiter\" em \";\" - preferência pessoal).\n",
    "\n",
    "**Informação importante**: Em cada célula, haverá um marcador - %%time - que irá informar quanto tempo levou para que cada operação seja realizada. É um dado importante, já que haverão comparações entre o tempo de execução do Pandas e do Polars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e701318a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 53.9 s\n",
      "Wall time: 54.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# DataFrame das eleições normais\n",
    "df_votacao = pd.read_csv('votacao_RJ_2022.csv', encoding='ISO-8859-1', delimiter=';')\n",
    "\n",
    "#DataFrame das eleições presidenciais\n",
    "df__votacao_presidente = pd.read_csv('votacao_BR_2022.csv', encoding='ISO-8859-1', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1a1e4",
   "metadata": {},
   "source": [
    "### Podemos concluir que:\n",
    "- Ambos os Dataframes posssuem 26 colunas - um número muito grande de colunas, sendo que não iremos precisar disso tudo\n",
    "- O número de linhas nos dois não será contabilizado, já que a natureza de coleta dos dados é diferente em ambos os datasets\n",
    "- Porém possuem a mesma estrutura (em nomes de colunas, natureza das informações e tipos primitivos) - o que será muito bom para a estratégia que vem a seguir\n",
    "- Não há valores nulos nas linhas e colunas - são dois DataFrames bem cuidados, aparentemente\n",
    "- Os tipos das colunas se dividem em \"int64\" e 'object' ('object' é a forma que o Pandas interpreta o tipo \"str\"). Talvez eu altere o tipo de dados para que o gasto de memória em cada operação fique 'redondinho', bem otimizado. No caso, é necessário? Como as operações então funcionando de forma fluida, eu diria que é mais para que o costume do \"tuning\" seja fixado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0369854c",
   "metadata": {},
   "source": [
    "## E qual vai ser a estratégia a partir de agora?\n",
    "\n",
    "A partir daqui, vamos realizar a fusão dos DataFrames, depois iremos salvá-los em um arquivo .xlsx. Depois dessa etapa, iremos re-invocar o novo arquivo para que possamos trabalhar em cima dele mais tranquilamente.\n",
    "\n",
    "## E por que fazer isso?\n",
    "\n",
    "Por alguns motivos.\n",
    "- O primeiro deles é por preguiça. E a preguiça, se bem aplicada, é um dos pilares da programação. Poupo a mim de escrever comandos redundantes e ainda por cima centralizo todos os dados em um só DataFrame. Mão na roda demais.\n",
    "- Os DataFrames possuem a mesma estrutura, sendo um pra eleição presidencial (nos dois turnos) e o outro pra todo o restante dos cargos - uma informação que acho pontual: No RJ, a eleição para governador se deu em apenas um turno. Houve o mesmo resultado em mais 14 estados.\n",
    "- Caso eu decida utilizar o dataset, já transformado do jeito que quero, como database em algum software de dataviz (Power BI, Qlik, Tableau, etc), não irei precisar utilizar várias bases de dados. Apenas uma já está de bom tamanho. Ajuda no consumo de memória.\n",
    "\n",
    "## Então vamos lá: Realizando a fusão dos DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ab3023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 891 ms\n",
      "Wall time: 899 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Realizando a concatenação dos DataFrames. Vamos lá aos argumentos:\n",
    "# axis = 0 - quero unir os DataFrames verticalmente, de acordo com as linhas\n",
    "# ignore_index=True - quero resetar o índice do novo DataFrame, evitando algumas 'doideras'\n",
    "\n",
    "df_eleicoes = pd.concat([df_votacao, df__votacao_presidente], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a9ce8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Analisando a saúde do novo DataFrame:\n",
    "\n",
    "- Mesmo número de colunas de antes, agora com o objeto tendo um pouco mais de 12 milhões de linhas\n",
    "- Mais uma vez, sem tipos nulos\n",
    "- Os tipos variam entre \"int64\" e \"object\"\n",
    "\n",
    "As colunas que ficarão no DataFrame serão:\n",
    "- 'NR_TURNO' (representa o turno da eleição)\n",
    "- 'NM_MUNICIPIO' - (nome do município, coluna mais importante por enquanto)\n",
    "- 'NR_ZONA' - (Zona eleitoral)\n",
    "- 'NR_SECAO' - (Seção eleitoral)\n",
    "- 'CD_CARGO - (Cargo do político)\n",
    "- 'NR_VOTAVEL' - (Número do político)\n",
    "- 'QT_VOTOS' - (Quantidade de votos que o político recebeu naquela zona/seção eleitoral)\n",
    "- 'NM_VOTAVEL' - (Nome do político)\n",
    "- 'NM_LOCAL_VOTACAO' - (Endereço do local da votação) ¹\n",
    "- 'DS_LOCAL_VOTACAO_ENDERECO' - (Nome do local da votação) ¹\n",
    "\n",
    "\n",
    "**Nota**: Percebe-se que há uma redução IMENSA a ser feita; Os outros dados não são tão relevantes pra essa EDA. Não estou tirando a importância dos dados que irei excluir; se eu quisesse fazer um modelo de machine learning, eu manteria o campo 'DT_ELEICAO', por exemplo. São ideias mais a frente.\n",
    "\n",
    "**Nota ¹**: As duas últimas colunas serão necessárias apenas temporariamente. Abaixo explico melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd6bf7",
   "metadata": {},
   "source": [
    "## Refatoração e reestruturação do DataFrame\n",
    "\n",
    "Como próxima etapa, irei primeiramente renomear as colunas do DataFrame principal para que eu possa ter uma padronização - parte de uma estratégia pra evitar ao máximo fazer coisas 'a mão'. Com as colunas renomeadas, irei selecionar apenas as informações pertinentes a Nova Friburgo - cidade a ser analisada. Por fim, vou fazer o casting das colunas numéricas do meu DataFrame principal - de int64 para int32.\n",
    "\n",
    "(**Meus dois centavos sobre o assunto**: Uma coisa que eu gostaria que fosse possível é o casting de colunas do tipo 'object' para 'str' com limitação de caracteres. No SQL mesmo, é possível modificar o tipo de uma coluna usando, no MySQL mesmo, *'MODIFY coluna VARCHAR(número)'*. Uma solução mais elegante para o casting. Nem tudo nessa vida é perfeito, infelizmente.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "872f5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lista com todas as colunas que preciso usar:\n",
    "colunas_a_manter = ['NR_TURNO',\n",
    "                    'NM_MUNICIPIO',\n",
    "                    'NR_ZONA',\n",
    "                    'NR_SECAO',\n",
    "                    'CD_CARGO',\n",
    "                    'NR_VOTAVEL',\n",
    "                    'QT_VOTOS',\n",
    "                    'NM_VOTAVEL',\n",
    "                    'NM_LOCAL_VOTACAO',\n",
    "                    'DS_LOCAL_VOTACAO_ENDERECO']\n",
    "\n",
    "# O novo DataFrame, com a lista sendo passada como argumento\n",
    "df_eleicoes = df_eleicoes[colunas_a_manter]\n",
    "df_eleicoes = df_eleicoes[df_eleicoes['NM_MUNICIPIO'] == 'NOVA FRIBURGO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "190dc697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 20.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Lista com as colunas que servirão como orientação para a ordenação de valores\n",
    "ordem = ['NR_ZONA','NR_SECAO']\n",
    "\n",
    "# Aqui é feita a ordenação do dataset\n",
    "df_eleicoes = df_eleicoes.sort_values(by=ordem,axis=0, ascending=True, kind='quicksort')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e3535",
   "metadata": {},
   "source": [
    "## Conversão dos tipos\n",
    "\n",
    "Aqui foi feita a conversão das colunas numéricas. De *'int 64'* para *'int32'*, foram testados vários algoritmos e as suas respesctivas velocidades. No final, com a ajuda do [José Paulo Costa](https://www.linkedin.com/in/josepaulocosta), o algoritmo escolhido foi implementado - utilizando uma máscara lógica e o método '.iloc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1b23b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 9.97 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Algoritmo de conversão otimizado:\n",
    "\n",
    "# Passo (1)\n",
    "mascara = (df_eleicoes.dtypes == 'int64').to_numpy()\n",
    "\n",
    "# Passo (2)\n",
    "df_eleicoes.iloc[:, mascara] = df_eleicoes.iloc[:, mascara].astype('int32', copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970a4ee",
   "metadata": {},
   "source": [
    "## Salvando o dataframe\n",
    "\n",
    "Vou precisar salvar o dataframe para poder extrair alguns dados dele, em outro notebook (bagunça demais fazer tudo em apenas um notebook). Será salvo em formato .xlsx, por comodidade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59c546ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eleicoes.to_excel('dataset_auxiliar.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73dc144",
   "metadata": {},
   "source": [
    "## Última etapa: Adição de alguns dados\n",
    "\n",
    "Irei precisar de duas tabelas-dimensão: Uma tabela de bairros e uma tabela de partidos políticos. Isso irá facilitar a análise violentamente. O código delas foi feito em outro notebook, por razões de organização. Neste notebook só iremos usar as tabelas já prontas.\n",
    "\n",
    "**E como essa facilitação vai rolar?** Fundindo as informações das duas tabelas dimensão com a tabela fato faz com que os dados fiquem centralizados em apenas uma tabela, evitando comandos de agregação *à posteriori* e, com isso, deixando o notebook mais \"limpo\". Essa é basicamente a última parte do *data munging*, posterior a isso faremos a análise de dados propriamente dita. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de85e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando a tabela de bairros\n",
    "lista_locais = pd.read_excel('lista_locais.xlsx')\n",
    "\n",
    "# Importando a tabela de partidos\n",
    "lista_partidos = pd.read_excel('lista_partidos.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daeb26b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Primeira fusão feita: Dataset principal com a lista de locais de votação\n",
    "df_eleicoes = df_eleicoes.merge(lista_locais, on='NM_LOCAL_VOTACAO', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b0eee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui tive que fazer um 'desvio': Obtive uma nova coluna a partir dos dois primeiros dígitos dos números dos políticos\n",
    "# Nova coluna obtida, fiz a conversão dela em 'int32' - já que ela foi convertida em str para a primeira manipulação\n",
    "df_eleicoes['NUMERO'] = df_eleicoes.NR_VOTAVEL.astype(str).str[:2]\n",
    "df_eleicoes['NUMERO'] = df_eleicoes['NUMERO'].astype('int32')\n",
    "\n",
    "# Realizando a segunda fusão: O dataframe, já fundido com a tabela de locais, com a tabela de partidos\n",
    "df_eleicoes = df_eleicoes.merge(lista_partidos, on='NUMERO', how='left')\n",
    "\n",
    "# Essa fusão gera uma 'poluição' na forma de colunas 'Unnamed'. Invoco uma lista com todas essas colunas, primeiramente\n",
    "colunas_irrelevantes = [cols for cols in df_eleicoes.columns if \"Unnamed\" in cols]\n",
    "\n",
    "# Depois, é só deletar essas colunas usando o método '.drop'\n",
    "df_eleicoes = df_eleicoes.drop(columns= colunas_irrelevantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff01dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora é eliminar as colunas originais que não serão usadas\n",
    "# Lista com todos os nomes das colunas\n",
    "colunas_irrelevantes = ['NM_MUNICIPIO', \n",
    "                        'NM_VOTAVEL', \n",
    "                        'NM_LOCAL_VOTACAO',\n",
    "                        'DS_LOCAL_VOTACAO_ENDERECO',\n",
    "                        'ZONA',\n",
    "                        'SECAO',\n",
    "                        'ENDERECO',\n",
    "                        'NUMERO',\n",
    "                        'PARTIDO'\n",
    "                       ]\n",
    "\n",
    "# O 'drop' das colunas é feito\n",
    "df_eleicoes = df_eleicoes.drop(columns=colunas_irrelevantes)\n",
    "\n",
    "# Por último, vamos renomear todas as colunas para que seja mais fácil na hora de fazer a EDA\n",
    "novos_nomes = ['TURNO','ZONA','SECAO','CARGO','NUMERO','VOTOS','BAIRRO','SIGLA','ALINHAMENTO']\n",
    "\n",
    "# Atribuindo os novos títulos das colunas ao dataframe\n",
    "df_eleicoes.columns = novos_nomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75180e5",
   "metadata": {},
   "source": [
    "## Adicionando mais algumas informações no dataframe\n",
    "\n",
    "Precisamos de mais uma coluna, que explicite se o voto ao candidato foi válido ou não\n",
    "E por que fazer isso? A urna eletrônica não contabiliza se o voto em candidaturas sub júdice (indeferidas a princípio mas que entraram com recurso) ou não, apenas computa o voto e essa análise é algo que fica a posteriori. Coluna importante caso queiramos fazer alguma análise dessas informações.\n",
    "\n",
    "Essas informações foram obtidas [nessa lista](https://sig.tse.jus.br/ords/dwapr/r/seai/sig-candidaturas/) e foram processadas em um outro notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0703f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\ASUS\\\\Desktop\\\\Projetos\\\\Portfólio\\\\Projeto Urna\\\\PASTA AUXILIAR\\\\indeferidos_final.xlsx'\n",
    "\n",
    "df_ind = pd.read_excel(path)\n",
    "\n",
    "indeferidos = df_ind['NUMERO'].tolist()\n",
    "COND = (df_eleicoes['NUMERO'].isin(indeferidos)) | ((df_eleicoes['NUMERO'] == 95) | (df_eleicoes['NUMERO'] == 96))\n",
    "\n",
    "\n",
    "df_eleicoes['SITUACAO'] = np.where(COND, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd862996",
   "metadata": {},
   "outputs": [],
   "source": [
    "COND2 = ((df_eleicoes['NUMERO'] == 95) | (df_eleicoes['NUMERO'] == 96))\n",
    "df_check = df_eleicoes[COND2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da1f312",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "E finalmente temos o nosso dataset pronto. \n",
    "\n",
    "Para fins de organização, vou utilizar o *dataset_eleicoes* **em outro notebook**. \n",
    "\n",
    "Particularmente foi mais fácil utilizar o Pandas por conta da quantidade de informações, tutoriais e vídeos internet afora explicando como utilizar cada comando. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebea1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o arquivo novo\n",
    "df_eleicoes.to_excel(\"dataset_eleicoes.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f460ab-a4b2-4cb0-8bf7-c735f75de9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
